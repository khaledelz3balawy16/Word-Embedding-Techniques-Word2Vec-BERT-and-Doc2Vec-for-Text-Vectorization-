{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7121520,"sourceType":"datasetVersion","datasetId":4107678}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Word Embedding Techniques (Word2Vec, BERT, and Doc2Vec) for Text Vectorization\r\n\r\nIn this notebook, we explore and apply three popular methods for generating vector representations (embeddings) of text data: **Word2Vec**, **BERT**, and **Doc2Vec**. These methods are essential for converting text into numerical vectors that can be used in various NLP tasks such as text classification, clustering, and more.\r\n\r\n1. **Word2Vec**:\r\n   - **Description**: Word2Vec is a neural network-based model that learns vector representations of words from large text corpora. It captures the semantic relationships between words by learning from context.\r\n   - **Advantages**:\r\n     - Flexible and customizable for specific tasks.\r\n     - Efficient for small to medium-sized datasets.\r\n   - **Disadvantages**:\r\n     - Slower when working with large datasets.\r\n     - May experience performance issues when adding too many columns to the DataFrame.\r\n   \r\n   **Note**: In this notebook, **Word2Vec** is used to generate word embeddings by training the model on the text data and then averaging word vectors to generate sentence embeddings.\r\n\r\n2. **BERT**:\r\n   - **Description**: BERT (Bidirectional Encoder Representations from Transformers) uses a transformer-based model to generate **contextualized embeddings**. Unlike traditional embeddings, BERT considers the surrounding context of words to generate more accurate representations.\r\n   - **Advantages**:\r\n     - Highly accurate and context-aware embeddings.\r\n     - Suitable for tasks requiring deep understanding of language context.\r\n   - **Disadvantages**:\r\n     - Computationally expensive and slower than other methods.\r\n     - Requires significant memory and resources, making it less efficient for very large datasets.\r\n   \r\n   **Note**: **BERT** is applied here to generate more context-aware sentence embeddings. These embeddings capture the meaning of words depending on their surrounding context.\r\n\r\n3. **Doc2Vec**:\r\n   - **Description**: Doc2Vec extends the Word2Vec approach by generating embeddings not just for words, but for entire documents or sentences. This method captures the overall meaning of larger text chunks, making it useful for document-level tasks.\r\n   - **Advantages**:\r\n     - Great for handling larger documents and capturing the meaning of whole sentences or paragraphs.\r\n     - Works well for tasks requiring the understanding of full text.\r\n   - **Disadvantages**:\r\n     - Training can be computationally intensive.\r\n     - May not offer as much control over individual words compared to **Word2Vec**.\r\n   \r\n   **Note**: **Doc2Vec** is used here to generate embeddings that represent entire documents or sentences, helping capture the semantic meaning beyond individual words.\r\n\r\n### **How to Use**:\r\n- **Word2Vec** is used for learning word embeddings and generating sentence representations by averaging the word vectors.\r\n- **BERT** is used to create contextualized embeddings, where the meaning of a word is influenced by the surrounding context.\r\n- **Doc2Vec** is applied to generate document-level embeddings, offering a higher-level representation of text.\r\n\r\nIn this notebook, we demonstrate how to use each of these methods to generate embeddings for text data and save the results as a CSV file for further analysis.\r\ns for further use in CSV format.\r\n","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/text-document-classification-dataset/df_file.csv\")\ndata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T06:35:19.916238Z","iopub.execute_input":"2024-12-06T06:35:19.917010Z","iopub.status.idle":"2024-12-06T06:35:20.069229Z","shell.execute_reply.started":"2024-12-06T06:35:19.916974Z","shell.execute_reply":"2024-12-06T06:35:20.068352Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                   Text  Label\n0     Budget to set scene for election\\n \\n Gordon B...      0\n1     Army chiefs in regiments decision\\n \\n Militar...      0\n2     Howard denies split over ID cards\\n \\n Michael...      0\n3     Observers to monitor UK election\\n \\n Minister...      0\n4     Kilroy names election seat target\\n \\n Ex-chat...      0\n...                                                 ...    ...\n2220  India opens skies to competition\\n \\n India wi...      4\n2221  Yukos bankruptcy 'not US matter'\\n \\n Russian ...      4\n2222  Survey confirms property slowdown\\n \\n Governm...      4\n2223  High fuel prices hit BA's profits\\n \\n British...      4\n2224  US trade gap hits record in 2004\\n \\n The gap ...      4\n\n[2225 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Budget to set scene for election\\n \\n Gordon B...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Army chiefs in regiments decision\\n \\n Militar...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Howard denies split over ID cards\\n \\n Michael...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Observers to monitor UK election\\n \\n Minister...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Kilroy names election seat target\\n \\n Ex-chat...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2220</th>\n      <td>India opens skies to competition\\n \\n India wi...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2221</th>\n      <td>Yukos bankruptcy 'not US matter'\\n \\n Russian ...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2222</th>\n      <td>Survey confirms property slowdown\\n \\n Governm...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2223</th>\n      <td>High fuel prices hit BA's profits\\n \\n British...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2224</th>\n      <td>US trade gap hits record in 2004\\n \\n The gap ...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>2225 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport gensim\nimport nltk\nimport torch\nfrom tqdm import tqdm\nfrom transformers import BertTokenizer, AutoModel\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom nltk.tokenize import word_tokenize\n\n# Initialize BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n\nnltk.download('punkt')  # Required for Doc2Vec tokenization\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T06:35:20.070856Z","iopub.execute_input":"2024-12-06T06:35:20.071516Z","iopub.status.idle":"2024-12-06T06:35:41.025203Z","shell.execute_reply.started":"2024-12-06T06:35:20.071475Z","shell.execute_reply":"2024-12-06T06:35:41.024271Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05eb53fb0289416ea3156f0b2248ab1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abc37dfeb8de416ca3d4d4db441ffdf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b01a93ae481c431e9401c37e242e0160"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4ab46a3980f4ce8804fb25381292c3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1567839598d44af9297d58e2519aadc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81f0bac8299941d78ab1a431657e896e"}},"metadata":{}},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Word2Vec","metadata":{}},{"cell_type":"markdown","source":"### الكود الأول Word2Vec(الكود القديم):\r\n#### - يعمل على إنشاء قاموس مخصص للكلمات باستخدام Word2Vec ويقوم بحساب المتوسط.\r\n#### - It creates a custom dictionary of words using Word2Vec and calculates the average.\r\n\r\n#### - مرن في التعامل مع النصوص ويمكنك تخصيصه حسب احتياجاتك.\r\n#### - Flexible in handling text and can be customized according to your needs.\r\n\r\n#### - بطيء نسبيًا عند التعامل مع بيانات ضخمة بسبب الحاجة إلى بناء نموذج Word2Vec من البداية.\r\n#### - It's relatively slow when dealing with large datasets because it requires building the Word2Vec model from scratch.\r\n\r\n#### - قد يتسبب في تحذيرات تتعلق بالأداء عندما يتم إضافة العديد من الأعمدة في DataFrame.\r\n#### - It may cause performance warnings when adding many columns to the DataFrame.\r\n\r\n#### Advantages:\r\n#### - More flexible and customizable to handle specific words or advanced text processing.\r\n#### - أكثر مرونة وقابلية للتخصيص للتعامل مع الكلمات المحددة أو المعالجة النصية المتقدمة.\r\n\r\n#### - Good for projects where you need to work with smaller, manageable datasets.\r\n#### - جيد للمشاريع التي تحتاج إلى التعامل مع مجموعات بيانات أصغر يمكن التحكم فيها.\r\n\r\n#### - Useful when you want more control over how word vectors are handled.\r\n#### - مفيد عندما ترغب في مزيد من التحكم في كيفية التعامل مع المتجهات الخاصة بالكلمات.\r\n\r\n#### Disadvantages:\r\n#### - Slower when working with large datasets because it builds the Word2Vec model from scratch.\r\n#### - أبطأ عند التعامل مع مجموعات بيانات ضخمة لأنه يقوم ببناء نموذج Word2Vec من البداية.\r\n\r\n#### - Performance warnings may appear when adding too many columns to the DataFrame.\r\n#### - قد تظهر تحذيرات من الأداء عند إضافة العديد من الأعمدة إلى DataFrame.\r\ne DataFrame.\r\nnced processing is needed.\r\n\r\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport gensim\n\ndef W2VAverage(Data, Feature, VectorSize=100):\n    # Create an empty DataFrame to store Word2Vec vectors\n    W2VDF = pd.DataFrame()\n\n    # Extract the text data from the 'Feature' column of the input 'Data'\n    Text = Data[Feature].tolist()\n\n    # Split the text into lists of words\n    Text = [str(i).split() for i in Text]\n\n    # Train a Word2Vec model on the text data with the specified vector size\n    model = gensim.models.Word2Vec(Text, vector_size=VectorSize)\n\n    # Populate the 'W2VDF' DataFrame with words and their corresponding Word2Vec vectors\n    W2VDF['Words'] = list(model.wv.key_to_index.keys())\n    W2VDF['W2V'] = W2VDF['Words'].apply(lambda x: model.wv.get_vector(x))\n\n    # Create a dictionary to map words to their Word2Vec vectors\n    W2VDict = {i: j for i, j in zip(W2VDF['Words'].tolist(), W2VDF['W2V'].tolist())}\n\n    # Define a function to apply Word2Vec averaging to text data\n    def ApplyW2V(x):\n        L = []\n        x = str(x).lower()\n\n        # Iterate through words in the text\n        for w in x.split():\n            if w in W2VDict.keys():\n                L.append(W2VDict[w])\n\n        # If there are Word2Vec vectors for the words in the text, calculate the mean\n        if len(L) > 0:\n            d = pd.DataFrame(L)\n            return d.mean(axis=0).tolist()  # Ensure this is returned as a list of 100 elements\n        else:\n            return None  # Return None if no match\n\n    # Apply Word2Vec averaging to the 'Feature' column of the 'Data'\n    Data['W2V'] = Data[Feature].apply(ApplyW2V)\n\n    # Remove rows where the Word2Vec vector is None (i.e., no words matched)\n    Data = Data.dropna(subset=['W2V'])\n\n    # Convert the vectors to separate columns (one column per dimension)\n    W2VColumns = pd.DataFrame(Data['W2V'].tolist(), columns=[f'C{i+1}' for i in range(VectorSize)])\n\n    # Add the words to the final output\n    W2VColumns['Word'] = Data[Feature].values\n\n    # Return the DataFrame with words and their corresponding vectors\n    return W2VColumns\n\n# Example of calling the function with your dataset\nW2VAverageData = W2VAverage(data, 'Text', VectorSize=100)\n\n# Save the output to a CSV file if you are working on Kaggle\nW2VAverageData.to_csv('W2VAverageData_output.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T06:35:41.026815Z","iopub.execute_input":"2024-12-06T06:35:41.027558Z","iopub.status.idle":"2024-12-06T06:36:03.871619Z","shell.execute_reply.started":"2024-12-06T06:35:41.027514Z","shell.execute_reply":"2024-12-06T06:36:03.870891Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\nW2VAverageData.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T06:36:03.873260Z","iopub.execute_input":"2024-12-06T06:36:03.873558Z","iopub.status.idle":"2024-12-06T06:36:03.896233Z","shell.execute_reply.started":"2024-12-06T06:36:03.873529Z","shell.execute_reply":"2024-12-06T06:36:03.895373Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"         C1        C2        C3        C4        C5        C6        C7  \\\n0 -0.131122  0.132420  0.371762  0.085390  0.198889 -0.208540 -0.092727   \n1 -0.130588  0.060809  0.374329  0.141239  0.206432 -0.139559 -0.063602   \n2 -0.009661  0.073400  0.422543  0.052059  0.133188 -0.100056 -0.145044   \n3 -0.055933  0.106168  0.351832  0.041982  0.144685 -0.214028 -0.159773   \n4  0.049851  0.094148  0.272093  0.044148  0.095214 -0.145893 -0.133750   \n\n         C8        C9       C10  ...       C92       C93       C94       C95  \\\n0  0.779647 -0.289085 -0.236045  ...  0.383347  0.107613 -0.360982  0.737224   \n1  0.753321 -0.282003 -0.189063  ...  0.393399  0.173181 -0.299578  0.752268   \n2  0.818228 -0.079964 -0.295547  ...  0.339059  0.091743 -0.415100  0.779377   \n3  0.783564 -0.268072 -0.314375  ...  0.338897  0.062605 -0.331467  0.653366   \n4  0.794728 -0.271272 -0.262693  ...  0.294523 -0.023523 -0.466268  0.718938   \n\n        C96       C97       C98       C99      C100  \\\n0  0.098876 -0.072426 -0.164225  0.330256 -0.140899   \n1  0.115492 -0.111245 -0.150251  0.349589 -0.160532   \n2  0.220381 -0.076980  0.072737  0.263418 -0.074121   \n3  0.140105 -0.063807 -0.081929  0.290400 -0.092682   \n4  0.157991  0.007200 -0.014884  0.256474 -0.098263   \n\n                                                Word  \n0  Budget to set scene for election\\n \\n Gordon B...  \n1  Army chiefs in regiments decision\\n \\n Militar...  \n2  Howard denies split over ID cards\\n \\n Michael...  \n3  Observers to monitor UK election\\n \\n Minister...  \n4  Kilroy names election seat target\\n \\n Ex-chat...  \n\n[5 rows x 101 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>C1</th>\n      <th>C2</th>\n      <th>C3</th>\n      <th>C4</th>\n      <th>C5</th>\n      <th>C6</th>\n      <th>C7</th>\n      <th>C8</th>\n      <th>C9</th>\n      <th>C10</th>\n      <th>...</th>\n      <th>C92</th>\n      <th>C93</th>\n      <th>C94</th>\n      <th>C95</th>\n      <th>C96</th>\n      <th>C97</th>\n      <th>C98</th>\n      <th>C99</th>\n      <th>C100</th>\n      <th>Word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.131122</td>\n      <td>0.132420</td>\n      <td>0.371762</td>\n      <td>0.085390</td>\n      <td>0.198889</td>\n      <td>-0.208540</td>\n      <td>-0.092727</td>\n      <td>0.779647</td>\n      <td>-0.289085</td>\n      <td>-0.236045</td>\n      <td>...</td>\n      <td>0.383347</td>\n      <td>0.107613</td>\n      <td>-0.360982</td>\n      <td>0.737224</td>\n      <td>0.098876</td>\n      <td>-0.072426</td>\n      <td>-0.164225</td>\n      <td>0.330256</td>\n      <td>-0.140899</td>\n      <td>Budget to set scene for election\\n \\n Gordon B...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.130588</td>\n      <td>0.060809</td>\n      <td>0.374329</td>\n      <td>0.141239</td>\n      <td>0.206432</td>\n      <td>-0.139559</td>\n      <td>-0.063602</td>\n      <td>0.753321</td>\n      <td>-0.282003</td>\n      <td>-0.189063</td>\n      <td>...</td>\n      <td>0.393399</td>\n      <td>0.173181</td>\n      <td>-0.299578</td>\n      <td>0.752268</td>\n      <td>0.115492</td>\n      <td>-0.111245</td>\n      <td>-0.150251</td>\n      <td>0.349589</td>\n      <td>-0.160532</td>\n      <td>Army chiefs in regiments decision\\n \\n Militar...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.009661</td>\n      <td>0.073400</td>\n      <td>0.422543</td>\n      <td>0.052059</td>\n      <td>0.133188</td>\n      <td>-0.100056</td>\n      <td>-0.145044</td>\n      <td>0.818228</td>\n      <td>-0.079964</td>\n      <td>-0.295547</td>\n      <td>...</td>\n      <td>0.339059</td>\n      <td>0.091743</td>\n      <td>-0.415100</td>\n      <td>0.779377</td>\n      <td>0.220381</td>\n      <td>-0.076980</td>\n      <td>0.072737</td>\n      <td>0.263418</td>\n      <td>-0.074121</td>\n      <td>Howard denies split over ID cards\\n \\n Michael...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.055933</td>\n      <td>0.106168</td>\n      <td>0.351832</td>\n      <td>0.041982</td>\n      <td>0.144685</td>\n      <td>-0.214028</td>\n      <td>-0.159773</td>\n      <td>0.783564</td>\n      <td>-0.268072</td>\n      <td>-0.314375</td>\n      <td>...</td>\n      <td>0.338897</td>\n      <td>0.062605</td>\n      <td>-0.331467</td>\n      <td>0.653366</td>\n      <td>0.140105</td>\n      <td>-0.063807</td>\n      <td>-0.081929</td>\n      <td>0.290400</td>\n      <td>-0.092682</td>\n      <td>Observers to monitor UK election\\n \\n Minister...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.049851</td>\n      <td>0.094148</td>\n      <td>0.272093</td>\n      <td>0.044148</td>\n      <td>0.095214</td>\n      <td>-0.145893</td>\n      <td>-0.133750</td>\n      <td>0.794728</td>\n      <td>-0.271272</td>\n      <td>-0.262693</td>\n      <td>...</td>\n      <td>0.294523</td>\n      <td>-0.023523</td>\n      <td>-0.466268</td>\n      <td>0.718938</td>\n      <td>0.157991</td>\n      <td>0.007200</td>\n      <td>-0.014884</td>\n      <td>0.256474</td>\n      <td>-0.098263</td>\n      <td>Kilroy names election seat target\\n \\n Ex-chat...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 101 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"### الكود الثاني (Word2Vec المعدل):\r\n#### - أسهل وأسرع في الاستخدام، يقوم بحساب المتوسط مباشرة باستخدام Word2Vec.\r\n#### - Faster and more efficient, calculates the average directly using Word2Vec.\r\n\r\n#### - أقل مرونة من الكود الأول لأنه يعتمد على إعدادات ثابتة ولا يوفر خيارات تخصيص متقدمة.\r\n#### - Less flexible than the first code because it relies on fixed settings and doesn't offer advanced customization options.\r\n\r\n#### - أسرع في المعالجة مقارنة بالكود الأول.\r\n#### - Faster in processing compared to the first code.\r\n\r\n#### Advantages:\r\n#### - Faster and more efficient, especially for smaller datasets.\r\n#### - أسرع وأكثر كفاءة، خاصة في التعامل مع مجموعات البيانات الصغيرة.\r\n\r\n#### - Simple and easy to implement, providing quick results.\r\n#### - بسيط وسهل التنفيذ، يوفر نتائج سريعة.\r\n\r\n#### - Ideal for tasks that don’t require heavy customization or advanced configurations.\r\n#### - مثالي للمهام التي لا تتطلب تخصيصًا كبيرًا أو إعدادات متقدمة.\r\n\r\n#### Disadvantages:\r\n#### - May not handle large datasets as effectively if more advanced processing is needed.\r\n#### - قد لا يتعامل مع مجموعات البيانات الكبيرة بشكل فعال إذا كانت هناك حاجة إلى معالجة أكثر تقدمًا.\r\ne advanced processing is needed.\r\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport gensim\nimport numpy as np  # تأكد من استيراد مكتبة numpy لحساب المتوسط\n\ndef W2VAverage(Data, Feature, VectorSize=100):\n    \"\"\"\n    Function to convert text to Word2Vec vectors and calculate the mean vector for words.\n    دالة لتحويل النصوص إلى فكتورات باستخدام Word2Vec وحساب المتوسط لفكتورات الكلمات.\n    \n    Args:\n    - Data: DataFrame containing the text data.\n      الداتا التي تحتوي على النصوص.\n    - Feature: The name of the column containing the text data.\n      اسم العمود الذي يحتوي على النصوص.\n    - VectorSize: The desired size of the word vectors (default is 100).\n      حجم الفكتورات المطلوبة (افتراضيًا 100).\n    \n    Returns:\n    - A DataFrame with words and their corresponding average Word2Vec vectors.\n      دالة تُرجع DataFrame يحتوي على الكلمات وفكتوراتها المقابلة.\n    \"\"\"\n    \n    # Convert the text data into a list of words.\n    # تحويل النصوص إلى قائمة من الكلمات\n    Text = Data[Feature].apply(lambda x: str(x).split())\n    \n    # Train a Word2Vec model on the text data.\n    # تدريب نموذج Word2Vec على البيانات النصية\n    model = gensim.models.Word2Vec(Text, vector_size=VectorSize, window=5, min_count=1, workers=4)\n    \n    # Function to calculate the average Word2Vec vector for each text (list of words).\n    # دالة لحساب المتوسط لفكتورات Word2Vec لكل نص (قائمة من الكلمات)\n    def ApplyW2V(x):\n        vectors = []  # Initialize an empty list to store the vectors\n        for word in x:  # Iterate through each word in the text\n            if word in model.wv:  # Check if the word is in the Word2Vec model's vocabulary\n                vectors.append(model.wv[word])  # Append the word vector\n        \n        # If we have vectors, calculate the mean vector across all words.\n        # إذا كان لدينا فكتورات، نحسب المتوسط عبر كل الكلمات\n        if vectors:\n            return pd.Series(np.mean(vectors, axis=0))  # Use numpy's mean to calculate the average\n        else:\n            return pd.Series([None] * VectorSize)  # If no words are found, return a series with None\n    \n    # Apply the ApplyW2V function to the text data.\n    # تطبيق الدالة ApplyW2V على بيانات النصوص\n    W2VData = Text.apply(ApplyW2V)\n    \n    # Add the original words column to the result.\n    # إضافة عمود الكلمات الأصلية إلى النتائج\n    W2VData['Word'] = Data[Feature].values\n    \n    return W2VData\n\n# Example usage of the function:\n# W2VAverageData = W2VAverage(data, 'Text', VectorSize=100)\n\n# To save the output to a CSV file:\n# W2VAverageData.to_csv('output.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T06:36:03.897475Z","iopub.execute_input":"2024-12-06T06:36:03.897961Z","iopub.status.idle":"2024-12-06T06:36:03.909065Z","shell.execute_reply.started":"2024-12-06T06:36:03.897920Z","shell.execute_reply":"2024-12-06T06:36:03.908254Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# مثال لاستخدام الدالة مع بيانات معينة:\nW2VAverageData = W2VAverage(data, 'Text', VectorSize=100)\n\n# لحفظ النتائج في ملف CSV:\nW2VAverageData.to_csv('output.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T06:36:03.910176Z","iopub.execute_input":"2024-12-06T06:36:03.910567Z","iopub.status.idle":"2024-12-06T06:36:10.241827Z","shell.execute_reply.started":"2024-12-06T06:36:03.910529Z","shell.execute_reply":"2024-12-06T06:36:10.241132Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Bert\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom transformers import BertTokenizer, BertModel\nfrom tqdm import tqdm\n\n# تحميل الـ Tokenizer و الـ Model الخاص بـ BERT\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# دالة لحساب بارتينج الجمل وحفظ الخرج في ملف CSV\ndef bert_vectorize_and_save(Data, Feature, output_file='bert_output.csv'):\n    # استخراج الجمل من العمود المخصص للنصوص\n    sentences = Data[Feature].tolist()\n\n    # تطبيق التوكنيزر لتحويل الجمل إلى رموز\n    encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n\n    # استخدام موديل BERT لاستخراج الـ Embeddings\n    with torch.no_grad():\n        model_output = model(**encoded_input)\n\n    # حساب الـ Mean Pooling للـ Embeddings\n    sentence_embeddings = model_output.last_hidden_state.mean(dim=1)\n\n    # تحويل الـ Embeddings إلى DataFrame\n    embeddings_df = pd.DataFrame(sentence_embeddings.numpy())\n\n    # التحقق إذا كان عمود \"Label\" موجود في البيانات قبل إضافته\n    if 'Label' in Data.columns:\n        embeddings_df['Label'] = Data['Label']\n    \n    # حفظ النتيجة في ملف CSV\n    embeddings_df.to_csv(output_file, index=False)\n\n    return embeddings_df\n\n# مثال على كيفية استدعاء الدالة وحفظ الخرج\n# تأكد من أن لديك الـ Data المناسبة قبل استدعاء الدالة\n# مثال:\n# Data = pd.read_csv('your_dataset.csv')\n# bert_vectorize_and_save(Data, 'Text', 'output_file.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T06:36:10.243733Z","iopub.execute_input":"2024-12-06T06:36:10.244117Z","iopub.status.idle":"2024-12-06T06:36:13.309548Z","shell.execute_reply.started":"2024-12-06T06:36:10.244078Z","shell.execute_reply":"2024-12-06T06:36:13.308889Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb9c9b46f7614c01bd76daf641f1aca1"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"bert_vectorize_and_save(data, 'Text', 'output_file.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T06:36:13.310463Z","iopub.execute_input":"2024-12-06T06:36:13.310703Z","iopub.status.idle":"2024-12-06T06:41:24.000687Z","shell.execute_reply.started":"2024-12-06T06:36:13.310679Z","shell.execute_reply":"2024-12-06T06:41:23.999820Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"             0         1         2         3         4         5         6  \\\n0    -0.260485 -0.249689  0.575901 -0.277601 -0.003355 -0.030696  0.077894   \n1    -0.166651 -0.069870  0.529241 -0.580869 -0.077211 -0.183808  0.123304   \n2    -0.248155 -0.358391  0.206174 -0.411694  0.015422  0.144521  0.009294   \n3    -0.120750 -0.306673  0.436360 -0.418010  0.000711 -0.149914  0.158809   \n4    -0.299933 -0.382835  0.698436 -0.424881  0.161340  0.051376  0.012397   \n...        ...       ...       ...       ...       ...       ...       ...   \n2220 -0.202220 -0.312977  0.415126  0.032931  0.425125 -0.150159 -0.092723   \n2221 -0.195824 -0.044644  0.174195  0.022286  0.101371 -0.275588 -0.167030   \n2222 -0.621629 -0.187879  0.391347  0.034913  0.273063  0.254223 -0.065127   \n2223 -0.364588 -0.113018  0.443435 -0.185876  0.238173  0.204525  0.120340   \n2224 -0.503242 -0.132616  0.562654  0.112850  0.012595  0.107365  0.152375   \n\n             7         8         9  ...       759       760       761  \\\n0     0.283541 -0.105402 -0.050896  ...  0.191574  0.318998 -0.012285   \n1     0.004518 -0.024211  0.002156  ...  0.106484  0.144022 -0.422713   \n2     0.159983 -0.042486  0.137489  ...  0.196633  0.142124 -0.202916   \n3     0.201255 -0.070015  0.195831  ...  0.282665  0.132892 -0.137338   \n4     0.342028 -0.128560 -0.091923  ...  0.323920  0.122669 -0.092493   \n...        ...       ...       ...  ...       ...       ...       ...   \n2220  0.629078  0.125659 -0.077277  ...  0.144521  0.127036 -0.156167   \n2221  0.251951  0.057936  0.306489  ...  0.033705  0.170378 -0.464790   \n2222  0.430357 -0.058636  0.183197  ...  0.136774  0.074971 -0.350466   \n2223  0.348188  0.203169  0.262348  ...  0.311545  0.104997 -0.266138   \n2224  0.417795 -0.344737  0.150123  ...  0.136663 -0.105987 -0.276561   \n\n           762       763       764       765       766       767  Label  \n0     0.138120 -0.562039  0.168880 -0.300333 -0.086521  0.062143      0  \n1     0.007752 -0.464773 -0.234798 -0.179947 -0.086757  0.309456      0  \n2     0.168043 -0.573460  0.102079 -0.150089 -0.255482  0.333226      0  \n3     0.033390 -0.656017  0.055683 -0.230324 -0.158656  0.195435      0  \n4     0.178999 -0.362245  0.168734 -0.097546 -0.138226  0.071983      0  \n...        ...       ...       ...       ...       ...       ...    ...  \n2220  0.122648 -0.124463 -0.054310 -0.160410  0.318539 -0.272514      4  \n2221 -0.058136 -0.357500  0.121089 -0.109391  0.240786  0.090840      4  \n2222  0.223838 -0.549659 -0.174912 -0.282130  0.319555 -0.103630      4  \n2223  0.319714 -0.510037 -0.129523 -0.191244  0.431673 -0.003329      4  \n2224  0.430927 -0.485497 -0.109495 -0.009787  0.454716  0.030008      4  \n\n[2225 rows x 769 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.260485</td>\n      <td>-0.249689</td>\n      <td>0.575901</td>\n      <td>-0.277601</td>\n      <td>-0.003355</td>\n      <td>-0.030696</td>\n      <td>0.077894</td>\n      <td>0.283541</td>\n      <td>-0.105402</td>\n      <td>-0.050896</td>\n      <td>...</td>\n      <td>0.191574</td>\n      <td>0.318998</td>\n      <td>-0.012285</td>\n      <td>0.138120</td>\n      <td>-0.562039</td>\n      <td>0.168880</td>\n      <td>-0.300333</td>\n      <td>-0.086521</td>\n      <td>0.062143</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.166651</td>\n      <td>-0.069870</td>\n      <td>0.529241</td>\n      <td>-0.580869</td>\n      <td>-0.077211</td>\n      <td>-0.183808</td>\n      <td>0.123304</td>\n      <td>0.004518</td>\n      <td>-0.024211</td>\n      <td>0.002156</td>\n      <td>...</td>\n      <td>0.106484</td>\n      <td>0.144022</td>\n      <td>-0.422713</td>\n      <td>0.007752</td>\n      <td>-0.464773</td>\n      <td>-0.234798</td>\n      <td>-0.179947</td>\n      <td>-0.086757</td>\n      <td>0.309456</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.248155</td>\n      <td>-0.358391</td>\n      <td>0.206174</td>\n      <td>-0.411694</td>\n      <td>0.015422</td>\n      <td>0.144521</td>\n      <td>0.009294</td>\n      <td>0.159983</td>\n      <td>-0.042486</td>\n      <td>0.137489</td>\n      <td>...</td>\n      <td>0.196633</td>\n      <td>0.142124</td>\n      <td>-0.202916</td>\n      <td>0.168043</td>\n      <td>-0.573460</td>\n      <td>0.102079</td>\n      <td>-0.150089</td>\n      <td>-0.255482</td>\n      <td>0.333226</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.120750</td>\n      <td>-0.306673</td>\n      <td>0.436360</td>\n      <td>-0.418010</td>\n      <td>0.000711</td>\n      <td>-0.149914</td>\n      <td>0.158809</td>\n      <td>0.201255</td>\n      <td>-0.070015</td>\n      <td>0.195831</td>\n      <td>...</td>\n      <td>0.282665</td>\n      <td>0.132892</td>\n      <td>-0.137338</td>\n      <td>0.033390</td>\n      <td>-0.656017</td>\n      <td>0.055683</td>\n      <td>-0.230324</td>\n      <td>-0.158656</td>\n      <td>0.195435</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.299933</td>\n      <td>-0.382835</td>\n      <td>0.698436</td>\n      <td>-0.424881</td>\n      <td>0.161340</td>\n      <td>0.051376</td>\n      <td>0.012397</td>\n      <td>0.342028</td>\n      <td>-0.128560</td>\n      <td>-0.091923</td>\n      <td>...</td>\n      <td>0.323920</td>\n      <td>0.122669</td>\n      <td>-0.092493</td>\n      <td>0.178999</td>\n      <td>-0.362245</td>\n      <td>0.168734</td>\n      <td>-0.097546</td>\n      <td>-0.138226</td>\n      <td>0.071983</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2220</th>\n      <td>-0.202220</td>\n      <td>-0.312977</td>\n      <td>0.415126</td>\n      <td>0.032931</td>\n      <td>0.425125</td>\n      <td>-0.150159</td>\n      <td>-0.092723</td>\n      <td>0.629078</td>\n      <td>0.125659</td>\n      <td>-0.077277</td>\n      <td>...</td>\n      <td>0.144521</td>\n      <td>0.127036</td>\n      <td>-0.156167</td>\n      <td>0.122648</td>\n      <td>-0.124463</td>\n      <td>-0.054310</td>\n      <td>-0.160410</td>\n      <td>0.318539</td>\n      <td>-0.272514</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2221</th>\n      <td>-0.195824</td>\n      <td>-0.044644</td>\n      <td>0.174195</td>\n      <td>0.022286</td>\n      <td>0.101371</td>\n      <td>-0.275588</td>\n      <td>-0.167030</td>\n      <td>0.251951</td>\n      <td>0.057936</td>\n      <td>0.306489</td>\n      <td>...</td>\n      <td>0.033705</td>\n      <td>0.170378</td>\n      <td>-0.464790</td>\n      <td>-0.058136</td>\n      <td>-0.357500</td>\n      <td>0.121089</td>\n      <td>-0.109391</td>\n      <td>0.240786</td>\n      <td>0.090840</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2222</th>\n      <td>-0.621629</td>\n      <td>-0.187879</td>\n      <td>0.391347</td>\n      <td>0.034913</td>\n      <td>0.273063</td>\n      <td>0.254223</td>\n      <td>-0.065127</td>\n      <td>0.430357</td>\n      <td>-0.058636</td>\n      <td>0.183197</td>\n      <td>...</td>\n      <td>0.136774</td>\n      <td>0.074971</td>\n      <td>-0.350466</td>\n      <td>0.223838</td>\n      <td>-0.549659</td>\n      <td>-0.174912</td>\n      <td>-0.282130</td>\n      <td>0.319555</td>\n      <td>-0.103630</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2223</th>\n      <td>-0.364588</td>\n      <td>-0.113018</td>\n      <td>0.443435</td>\n      <td>-0.185876</td>\n      <td>0.238173</td>\n      <td>0.204525</td>\n      <td>0.120340</td>\n      <td>0.348188</td>\n      <td>0.203169</td>\n      <td>0.262348</td>\n      <td>...</td>\n      <td>0.311545</td>\n      <td>0.104997</td>\n      <td>-0.266138</td>\n      <td>0.319714</td>\n      <td>-0.510037</td>\n      <td>-0.129523</td>\n      <td>-0.191244</td>\n      <td>0.431673</td>\n      <td>-0.003329</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2224</th>\n      <td>-0.503242</td>\n      <td>-0.132616</td>\n      <td>0.562654</td>\n      <td>0.112850</td>\n      <td>0.012595</td>\n      <td>0.107365</td>\n      <td>0.152375</td>\n      <td>0.417795</td>\n      <td>-0.344737</td>\n      <td>0.150123</td>\n      <td>...</td>\n      <td>0.136663</td>\n      <td>-0.105987</td>\n      <td>-0.276561</td>\n      <td>0.430927</td>\n      <td>-0.485497</td>\n      <td>-0.109495</td>\n      <td>-0.009787</td>\n      <td>0.454716</td>\n      <td>0.030008</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>2225 rows × 769 columns</p>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom gensim.models import Doc2Vec\nfrom tqdm import tqdm\nfrom gensim.models.doc2vec import TaggedDocument\n\ndef ApplyDoc2Vec(Data, Feature, VectorSize=300, batch_size=8, output_file='doc2vec_output.csv'):\n    # Preprocess and clean the text data by replacing double newline characters with a space\n    AllReviews = Data[Feature].str.replace('\\n\\n', ' ')\n\n    # Add an 'Index' column to the input 'Data'\n    Data['Index'] = list(Data.index)\n\n    # Check if 'Label' column exists in the dataset\n    if 'Label' not in Data.columns:\n        raise KeyError(\"'Label' column is missing in the DataFrame.\")\n\n    # Filter out rows with non-null values in 'AllReviews'\n    non_na = AllReviews.notna()\n    non_na_Reviews = AllReviews[non_na]\n    non_na_Index = Data[non_na]['Index']\n\n    # Tokenize the non-null reviews and create a list of TaggedDocument objects\n    non_na_reviews_list = list(map(lambda x: word_tokenize(x), non_na_Reviews.values))\n    documents = [TaggedDocument(doc, [non_na_Index.values[i]]) for i, doc in enumerate(non_na_reviews_list)]\n\n    # Train a Doc2Vec model with the specified vector size and other settings\n    model = Doc2Vec(documents, vector_size=VectorSize, window=2, min_count=1, workers=4)\n\n    # Create a dictionary to map document indices to their embeddings\n    document_dict = {}\n    all_embeddings = []\n\n    # Process in batches for performance optimization\n    for i in tqdm(range(0, len(Data), batch_size)):\n        batch_indices = Data.iloc[i:i + batch_size]['Index']\n        for idx in batch_indices:\n            text = Data.loc[Data['Index'] == idx, Feature].values[0].replace(\"\\n\\n\", ' ')\n            document_dict[idx] = model.dv[idx]  # model.docvecs[idx] is now model.dv[idx] in newer gensim versions\n            all_embeddings.append(model.dv[idx])  # Append embeddings to the list\n\n    # Convert the embeddings to a DataFrame\n    document_embeddings_df = pd.DataFrame(all_embeddings)\n\n    # Add 'Label' column from the original DataFrame\n    document_embeddings_df['Label'] = Data['Label'].loc[document_embeddings_df.index]\n\n    # Save the embeddings DataFrame to CSV\n    document_embeddings_df.to_csv(output_file, index=False)\n\n    # Return the embeddings DataFrame\n    return document_embeddings_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T06:46:39.594314Z","iopub.execute_input":"2024-12-06T06:46:39.595138Z","iopub.status.idle":"2024-12-06T06:46:39.603589Z","shell.execute_reply.started":"2024-12-06T06:46:39.595100Z","shell.execute_reply":"2024-12-06T06:46:39.602632Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\n# تطبيق الدالة وحفظ النتائج في ملف\nresult_df = ApplyDoc2Vec(data, 'Text', VectorSize=300, batch_size=8, output_file='doc2vec_output.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T06:46:43.627979Z","iopub.execute_input":"2024-12-06T06:46:43.628336Z","iopub.status.idle":"2024-12-06T06:47:01.540064Z","shell.execute_reply.started":"2024-12-06T06:46:43.628304Z","shell.execute_reply":"2024-12-06T06:47:01.539267Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 279/279 [00:00<00:00, 445.15it/s]\n","output_type":"stream"}],"execution_count":17}]}